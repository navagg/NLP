{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the second Part of Char level Modeling and will discuss One Layer Neural Model for same\n",
    "### Complete First Part before starting this\n",
    "By Naveen Aggarwal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Step in developing a NN Model is to create a training set. In this case, all bigrams will be part of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indian Names: NamesIndian.txt, Foreign Names: names.txt\n",
    "words=open(\"NamesIndian.txt\", 'r').read().splitlines()\n",
    "# If we read indian names, Then we have to do some extra work in next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaban', 'aabharan', 'aabhas', 'aabhat', 'aabheer', 'aabheer', 'abheer', 'aabher', 'aabi', 'aabilesh']\n",
      "['aaban', 'aabharan', 'aabhas', 'aabhat', 'aabheer', 'aabheer', 'abheer', 'aabher', 'aabi', 'aabilesh']\n"
     ]
    }
   ],
   "source": [
    "# keep everything in lower case, strip blank spacesm replace - and ''\n",
    "import re\n",
    "print(words[:10])\n",
    "a = (map(lambda x: x.lower().strip().replace('-','',1).replace(' ','',1), words))\n",
    "#a_new=re.sub('[ -]','',a)\n",
    "words = list(a)\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '.', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "#chars= list('.ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')\n",
    "#start_token='.'\n",
    "# Char list created in this way. As extra character was creating problem.\n",
    "\n",
    "chars= sorted(set(\"\".join([start_token]+ words )))\n",
    "print(chars)\n",
    "stoi={s:i for i,s in enumerate(chars)}\n",
    "#stoi\n",
    "\n",
    "# Lets also create itos, we will use later\n",
    "itos={i:s for i,s in enumerate (chars)}\n",
    "#itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". a\n",
      "a a\n",
      "a b\n",
      "b a\n",
      "a n\n",
      "n .\n",
      "tensor([ 1,  2,  2,  3,  2, 15]) tensor([ 2,  2,  3,  2, 15,  1])\n"
     ]
    }
   ],
   "source": [
    "# Create a Training set and run for first word to test the code\n",
    "xs, ys = [], []\n",
    "for w in words [:1]:\n",
    "    ws=  [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip (ws[:], ws[1:]):\n",
    "        ix1=stoi[ch1]\n",
    "        ix2=stoi[ch2]\n",
    "        print (ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "# Convert into tensors\n",
    "\n",
    "xs=torch.tensor(xs)\n",
    "ys=torch.tensor(ys)\n",
    "print(xs, ys)\n",
    "\n",
    "# You can check how traing set is created, For 0 value in xs, it is 5 in ys. For 5 value in xs, it is 13 in ys. \n",
    "# Similar to bigrams are read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now to use this into the NN Model, We have to convert these integers into one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use one hot encoding function of torch\n",
    "import torch.nn.functional as f\n",
    "\n",
    "# num_classes is important to provide, otherwise it will take largest number as \"No of classes\". \n",
    "#In Neural network, we want to use Float values\n",
    "\n",
    "xhot=f.one_hot(xs, num_classes=28).float()\n",
    "#xhot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Initalize the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes depends upon total character in the matrix. Due to extra character, Number of classes are 28\n",
    "W=torch.randn((28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we use generator, then we can generate same random numbers again. This is helpful, when you are developing and testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "g=torch.Generator().manual_seed(214783647)\n",
    "W=torch.randn((28,28), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 28]) torch.Size([28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Lets See the size of xhot and W\n",
    "print (xhot.shape, W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 28])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Lets multiply, In torch use @ for multiplication\n",
    "logit=xhot@W\n",
    "logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 28])\n",
      "tensor(1.0000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# We will consider these as log count, So we will take exp. exp will be 0 to 1for negative numbers and greater than  1 for positive numbers\n",
    "counts=logit.exp()\n",
    "# This count we will take as equivalent to N\n",
    "# Now Probability is just a normalized count\n",
    "probs=counts /counts.sum(1, keepdims=True)\n",
    "print (probs.shape)\n",
    "print (probs[0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Lets combine above all steps and run it on all words, We will call it a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "xhot=f.one_hot(xs, num_classes=28).float()\n",
    "logit=xhot@W\n",
    "counts=logit.exp()\n",
    "probs=counts /counts.sum(1, keepdims=True)\n",
    "# We will define loss function as difference from actual value in ys\n",
    "loss= -probs[torch.arange(6),ys].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.243195533752441\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets write code for Backward Pass. In backward pass, we have to calculate the gradients and update the weights\n",
    "#### Since Pytorch makes the computation graph. If computations are differentiable, Then Pytorch calculate the back ward pass automatically. We have to just call the backward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass\n",
    "W.grad=None\n",
    "loss.backward()\n",
    "\n",
    "#Update\n",
    "W.data+=-(0.1)*W.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Lets execute these in loop for all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "505470\n"
     ]
    }
   ],
   "source": [
    "# create Dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    ws=  ws=[\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip (ws[:], ws[1:]):\n",
    "        ix1=stoi[ch1]\n",
    "        ix2=stoi[ch2]\n",
    "        #print (ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "# Convert into tensors\n",
    "\n",
    "xs=torch.tensor(xs)\n",
    "ys=torch.tensor(ys)\n",
    "num=xs.nelement()\n",
    "print(num)\n",
    "\n",
    "# initialize network\n",
    "g=torch.Generator().manual_seed(214783647)\n",
    "W=torch.randn((28,28), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.890211343765259\n",
      "3.25390362739563\n",
      "2.918787717819214\n",
      "2.7444732189178467\n",
      "2.63631534576416\n",
      "2.5683062076568604\n",
      "2.515385627746582\n",
      "2.4837374687194824\n",
      "2.451887845993042\n",
      "2.434147357940674\n",
      "2.410933494567871\n",
      "2.3999452590942383\n",
      "2.3812429904937744\n",
      "2.374174118041992\n",
      "2.358194351196289\n",
      "2.3536746501922607\n",
      "2.3395702838897705\n",
      "2.336874485015869\n",
      "2.3241775035858154\n",
      "2.3228671550750732\n",
      "2.3112704753875732\n",
      "2.311048746109009\n",
      "2.3003318309783936\n",
      "2.300985813140869\n",
      "2.290984630584717\n",
      "2.292355537414551\n",
      "2.2829442024230957\n",
      "2.284910202026367\n",
      "2.275989294052124\n",
      "2.278453826904297\n",
      "2.26994252204895\n",
      "2.2728261947631836\n",
      "2.2646591663360596\n",
      "2.2678956985473633\n",
      "2.260016918182373\n",
      "2.263551950454712\n",
      "2.255915880203247\n",
      "2.2597029209136963\n",
      "2.2522706985473633\n",
      "2.256272315979004\n",
      "2.2490122318267822\n",
      "2.2531964778900146\n",
      "2.246082305908203\n",
      "2.2504234313964844\n",
      "2.2434332370758057\n",
      "2.2479095458984375\n",
      "2.2410264015197754\n",
      "2.2456204891204834\n",
      "2.2388291358947754\n",
      "2.243525981903076\n",
      "2.2368149757385254\n",
      "2.2416021823883057\n",
      "2.23496150970459\n",
      "2.2398293018341064\n",
      "2.233250617980957\n",
      "2.238189697265625\n",
      "2.2316665649414062\n",
      "2.2366697788238525\n",
      "2.230196237564087\n",
      "2.235257387161255\n",
      "2.228828191757202\n",
      "2.2339413166046143\n",
      "2.227552652359009\n",
      "2.2327139377593994\n",
      "2.226360559463501\n",
      "2.2315657138824463\n",
      "2.225245237350464\n",
      "2.2304904460906982\n",
      "2.2241997718811035\n",
      "2.2294812202453613\n",
      "2.2232184410095215\n",
      "2.2285332679748535\n",
      "2.2222955226898193\n",
      "2.2276415824890137\n",
      "2.221426486968994\n",
      "2.2268006801605225\n",
      "2.220607042312622\n",
      "2.2260074615478516\n",
      "2.2198333740234375\n",
      "2.225257635116577\n",
      "2.2191011905670166\n",
      "2.224548101425171\n",
      "2.2184083461761475\n",
      "2.2238762378692627\n",
      "2.2177517414093018\n",
      "2.223238706588745\n",
      "2.217128038406372\n",
      "2.2226333618164062\n",
      "2.2165355682373047\n",
      "2.222057342529297\n",
      "2.2159712314605713\n",
      "2.221508741378784\n",
      "2.2154340744018555\n",
      "2.2209863662719727\n",
      "2.214921712875366\n",
      "2.220487356185913\n",
      "2.21443247795105\n",
      "2.220010995864868\n",
      "2.2139647006988525\n",
      "2.219555139541626\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    # Forward Pass\n",
    "    xhot=f.one_hot(xs, num_classes=28).float()\n",
    "    logit=xhot@W\n",
    "    counts=logit.exp()\n",
    "    probs=counts /counts.sum(1, keepdims=True)\n",
    "    # We will define loss function as difference from actual value in ys\n",
    "    loss= -probs[torch.arange(num),ys].log().mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # Backward Pass\n",
    "    W.grad=None\n",
    "    loss.backward()\n",
    "\n",
    "    #Update\n",
    "    #W.data+=-(0.1)*W.grad\n",
    "    W.data+=-(50)*W.grad\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets generate few names from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "umala.\n",
      "chan.\n",
      "lan.\n",
      "qbjrikrujthuthi.\n",
      "oradan.\n",
      "q varanthinikan.\n",
      "chujathandaneshekand.\n",
      "ilxdhujkaneimhajeralvanak.\n",
      "chanshapi.\n",
      "mila.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    out=[]\n",
    "    ix=0\n",
    "    while True:\n",
    "        # Earlier we did this in previous Notebook\n",
    "        #p=P[ix]\n",
    "        # But now we will do this\n",
    "        \n",
    "        xhot=f.one_hot(torch.tensor([ix]), num_classes=28).float()\n",
    "        logit=xhot@W\n",
    "        counts=logit.exp()\n",
    "        p=counts /counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix=torch.multinomial(p, num_samples=1, replacement= True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 1:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
